{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "This time we employ the cross validation to figure out the best model for spam filter.\n",
    "\n",
    "1. Use the function **GridSearchCV** to find out the best combination of parameter for logistic regression.( Set *cv=5* and *scoring = 'accuracy'*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import grid_search\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_grid = [{'penalty': ['l1', 'l2'], 'fit_intercept': [False, True], \n",
    "              'C':np.logspace(-5, 5, 100)}]\n",
    "# para_grid = [{'penalty': ['l1', 'l2'], 'fit_intercept': [False, True], \n",
    "#               'C':np.logspace(-5, 5, 100), 'normalize':[True]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - What's the best combination?\n",
    "    - What's the best score?\n",
    "    - Refit the best estimator on the whole data set. How many coefficients are shrinked to 0?(Hint: the absolute value of coefficients that are smaller than 1e-4.) \n",
    "    - What's the corresponding training error and test error? (Training error is the model performance on spam_train, while test error is the performance on spam_test.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Best Score\n",
      "0.93\n",
      "----------------------------------------------------------------------\n",
      "Best Parameters\n",
      "{'penalty': 'l2', 'C': 1917.9102616724849, 'fit_intercept': True}\n",
      "----------------------------------------------------------------------\n",
      "Best Estimator\n",
      "LogisticRegression(C=1917.9102616724849, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "----------------------------------------------------------------------\n",
      "Refitting best estimator...\n",
      "----------------------------------------------------------------------\n",
      "Number of Coefficients Shrunk to 0 (abs(coef) < 1e-4): 0\n",
      "----------------------------------------------------------------------\n",
      "Training Error: 0.0648\n",
      "----------------------------------------------------------------------\n",
      "Test Error: 0.0717\n"
     ]
    }
   ],
   "source": [
    "logit = linear_model.LogisticRegression()\n",
    "\n",
    "train = pd.read_csv('spam_train.csv')\n",
    "test = pd.read_csv('spam_test.csv')\n",
    "\n",
    "train_x = train.iloc[:, :57]\n",
    "train_y = train.iloc[:, 57]\n",
    "\n",
    "test_x = test.iloc[:, :57]\n",
    "test_y = test.iloc[:, 57]\n",
    "\n",
    "para_search = grid_search.GridSearchCV(logit, para_grid, scoring='accuracy', cv =5).fit(train_x, train_y)\n",
    "\n",
    "# print \"-\" * 70\n",
    "# print \"Grid Scores\"\n",
    "# print para_search.grid_scores_\n",
    "print \"-\" * 70\n",
    "print \"Best Score\"\n",
    "print para_search.best_score_\n",
    "print \"-\" * 70\n",
    "print \"Best Parameters\"\n",
    "print para_search.best_params_\n",
    "print \"-\" * 70\n",
    "print \"Best Estimator\"\n",
    "print para_search.best_estimator_\n",
    "print \"-\" * 70\n",
    "print \"Refitting best estimator...\"\n",
    "logit_best = para_search.best_estimator_\n",
    "logit_best.fit(train_x, train_y)\n",
    "print \"-\" * 70\n",
    "print \"Number of Coefficients Shrunk to 0 (abs(coef) < 1e-4): %i\" %sum(abs(logit_best.coef_[0]) < 1e-4)\n",
    "print \"-\" * 70\n",
    "print \"Training Error: %.4f\" %(1 - logit_best.score(train_x, train_y))\n",
    "print \"-\" * 70\n",
    "print \"Test Error: %.4f\" %(1 - logit_best.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Set *scoring = 'roc_auc'* and search again, what's the best parameters? Fit the best estimator on the spam_train data set. What's the training error and test error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Best Score\n",
      "0.971554896013\n",
      "----------------------------------------------------------------------\n",
      "Best Parameters\n",
      "{'penalty': 'l1', 'C': 1.4174741629268048, 'fit_intercept': True}\n",
      "----------------------------------------------------------------------\n",
      "Best Estimator\n",
      "LogisticRegression(C=1.4174741629268048, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "----------------------------------------------------------------------\n",
      "Refitting best estimator...\n",
      "----------------------------------------------------------------------\n",
      "Number of Coefficients Shrunk to 0 (abs(coef) < 1e-4): 4\n",
      "----------------------------------------------------------------------\n",
      "Training Error: 0.0639\n",
      "----------------------------------------------------------------------\n",
      "Test Error: 0.0717\n"
     ]
    }
   ],
   "source": [
    "logit = linear_model.LogisticRegression()\n",
    "\n",
    "train = pd.read_csv('spam_train.csv')\n",
    "test = pd.read_csv('spam_test.csv')\n",
    "\n",
    "train_x = train.iloc[:, :57]\n",
    "train_y = train.iloc[:, 57]\n",
    "\n",
    "test_x = test.iloc[:, :57]\n",
    "test_y = test.iloc[:, 57]\n",
    "\n",
    "train_y = pd.Series([1 if x == 'spam' else 0 for x in train_y])\n",
    "test_y = pd.Series([1 if x == 'spam' else 0 for x in test_y])\n",
    "\n",
    "para_search = grid_search.GridSearchCV(logit, para_grid, scoring='roc_auc', cv =5).fit(train_x, train_y)\n",
    "\n",
    "# print \"-\" * 70\n",
    "# print \"Grid Scores\"\n",
    "# print para_search.grid_scores_\n",
    "print \"-\" * 70\n",
    "print \"Best Score\"\n",
    "print para_search.best_score_\n",
    "print \"-\" * 70\n",
    "print \"Best Parameters\"\n",
    "print para_search.best_params_\n",
    "print \"-\" * 70\n",
    "print \"Best Estimator\"\n",
    "print para_search.best_estimator_\n",
    "print \"-\" * 70\n",
    "print \"Refitting best estimator...\"\n",
    "logit_best = para_search.best_estimator_\n",
    "logit_best.fit(train_x, train_y)\n",
    "print \"-\" * 70\n",
    "print \"Number of Coefficients Shrunk to 0 (abs(coef) < 1e-4): %i\" %sum(abs(logit_best.coef_[0]) < 1e-4)\n",
    "print \"-\" * 70\n",
    "print \"Training Error: %.4f\" %(1 - logit_best.score(train_x, train_y))\n",
    "print \"-\" * 70\n",
    "print \"Test Error: %.4f\" %(1 - logit_best.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.In this exercise, we will predict the number of applications received(*Apps*) using the other variables in the College data set.\n",
    "\n",
    "The features and the target variable are prepared as $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Private</th>\n",
       "      <th>Apps</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Enroll</th>\n",
       "      <th>Top10perc</th>\n",
       "      <th>Top25perc</th>\n",
       "      <th>F.Undergrad</th>\n",
       "      <th>P.Undergrad</th>\n",
       "      <th>Outstate</th>\n",
       "      <th>Room.Board</th>\n",
       "      <th>Books</th>\n",
       "      <th>Personal</th>\n",
       "      <th>PhD</th>\n",
       "      <th>Terminal</th>\n",
       "      <th>S.F.Ratio</th>\n",
       "      <th>perc.alumni</th>\n",
       "      <th>Expend</th>\n",
       "      <th>Grad.Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1660</td>\n",
       "      <td>1232</td>\n",
       "      <td>721</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>2885</td>\n",
       "      <td>537</td>\n",
       "      <td>7440</td>\n",
       "      <td>3300</td>\n",
       "      <td>450</td>\n",
       "      <td>2200</td>\n",
       "      <td>70</td>\n",
       "      <td>78</td>\n",
       "      <td>18.1</td>\n",
       "      <td>12</td>\n",
       "      <td>7041</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>2186</td>\n",
       "      <td>1924</td>\n",
       "      <td>512</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>2683</td>\n",
       "      <td>1227</td>\n",
       "      <td>12280</td>\n",
       "      <td>6450</td>\n",
       "      <td>750</td>\n",
       "      <td>1500</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>12.2</td>\n",
       "      <td>16</td>\n",
       "      <td>10527</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1428</td>\n",
       "      <td>1097</td>\n",
       "      <td>336</td>\n",
       "      <td>22</td>\n",
       "      <td>50</td>\n",
       "      <td>1036</td>\n",
       "      <td>99</td>\n",
       "      <td>11250</td>\n",
       "      <td>3750</td>\n",
       "      <td>400</td>\n",
       "      <td>1165</td>\n",
       "      <td>53</td>\n",
       "      <td>66</td>\n",
       "      <td>12.9</td>\n",
       "      <td>30</td>\n",
       "      <td>8735</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>417</td>\n",
       "      <td>349</td>\n",
       "      <td>137</td>\n",
       "      <td>60</td>\n",
       "      <td>89</td>\n",
       "      <td>510</td>\n",
       "      <td>63</td>\n",
       "      <td>12960</td>\n",
       "      <td>5450</td>\n",
       "      <td>450</td>\n",
       "      <td>875</td>\n",
       "      <td>92</td>\n",
       "      <td>97</td>\n",
       "      <td>7.7</td>\n",
       "      <td>37</td>\n",
       "      <td>19016</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>193</td>\n",
       "      <td>146</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>249</td>\n",
       "      <td>869</td>\n",
       "      <td>7560</td>\n",
       "      <td>4120</td>\n",
       "      <td>800</td>\n",
       "      <td>1500</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "      <td>11.9</td>\n",
       "      <td>2</td>\n",
       "      <td>10922</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Private  Apps  Accept  Enroll  Top10perc  Top25perc  F.Undergrad  \\\n",
       "0     Yes  1660    1232     721         23         52         2885   \n",
       "1     Yes  2186    1924     512         16         29         2683   \n",
       "2     Yes  1428    1097     336         22         50         1036   \n",
       "3     Yes   417     349     137         60         89          510   \n",
       "4     Yes   193     146      55         16         44          249   \n",
       "\n",
       "   P.Undergrad  Outstate  Room.Board  Books  Personal  PhD  Terminal  \\\n",
       "0          537      7440        3300    450      2200   70        78   \n",
       "1         1227     12280        6450    750      1500   29        30   \n",
       "2           99     11250        3750    400      1165   53        66   \n",
       "3           63     12960        5450    450       875   92        97   \n",
       "4          869      7560        4120    800      1500   76        72   \n",
       "\n",
       "   S.F.Ratio  perc.alumni  Expend  Grad.Rate  \n",
       "0       18.1           12    7041         60  \n",
       "1       12.2           16   10527         56  \n",
       "2       12.9           30    8735         54  \n",
       "3        7.7           37   19016         59  \n",
       "4       11.9            2   10922         15  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college = pd.read_csv('college.csv')\n",
    "x = college.iloc[:, 2:]\n",
    "y = college.iloc[:, 1]\n",
    "college.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) Split this data into a training set and a test set with train_size=0.5.(Hint: Use the function **sklearn.cross_validation.train_test_split** , set *random_state=0* and *tran_size=0.5*.)\n",
    "\n",
    "- (2) Fit a linear model on the training set and report the training error and test error(mean squared error, you can use the function *sklearn.metrics.mean_squared_error*).\n",
    "\n",
    "- (3) Fit a ridge regression on the training set, with $\\alpha$ chosen by the cross validation. Report the training error and test error.\n",
    "\n",
    "- (4) Fit a lasso on the training set, with $\\alpha$ chosen by the cross validation. Report the training error and test error\n",
    "\n",
    "- (5) Compare the results obtained, what do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Linear Regression:\n",
      "Training MSE: 1113145.99308\n",
      "Test MSE: 1244800.26999\n",
      "----------------------------------------------------------------------\n",
      "Ridge Regression:\n",
      "Alpha: 0.01\n",
      "Training Error: 0.0734662143432\n",
      "Test Error: 0.0846372266906\n",
      "----------------------------------------------------------------------\n",
      "Lasso Regression:\n",
      "Alpha: 0.792482898354\n",
      "Training Error: 0.0734662281467\n",
      "Test Error: 0.0846293007955\n",
      "----------------------------------------------------------------------\n",
      "Given that the test error for Ridge and Lasso are pretty close, I would choose the Lasso Regression because it allows for feature selection\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state = 0)\n",
    "print \"-\" * 70\n",
    "print \"Linear Regression:\"\n",
    "cLR = LinearRegression()\n",
    "cLR.fit(x_train, y_train)\n",
    "cLRtrain = cLR.predict(x_train)\n",
    "cLRtest = cLR.predict(x_test)\n",
    "print \"Training MSE: %s\" %mean_squared_error(y_train, cLRtrain)\n",
    "print \"Test MSE: %s\" %mean_squared_error(y_test, cLRtest)\n",
    "\n",
    "print \"-\" * 70\n",
    "print \"Ridge Regression:\"\n",
    "cvAlpha = np.logspace(-2, 2, 100)\n",
    "cRidgeCV = RidgeCV(alphas=cvAlpha, normalize=True, fit_intercept=True, scoring='r2')\n",
    "cRidgeCV.fit(x_train, y_train)\n",
    "print \"Alpha: %s\" %cRidgeCV.alpha_\n",
    "cRidge = Ridge(alpha=cRidgeCV.alpha_)\n",
    "cRidge.fit(x_train, y_train)\n",
    "print \"Training Error: %s\" %(1-cRidge.score(x_train, y_train))\n",
    "print \"Test Error: %s\" %(1-cRidge.score(x_test, y_test))\n",
    "\n",
    "print \"-\" * 70\n",
    "print \"Lasso Regression:\"\n",
    "cLassoCV = LassoCV(alphas=cvAlpha, normalize=True, fit_intercept=True)\n",
    "cLassoCV.fit(x_train, y_train)\n",
    "print \"Alpha: %s\" %cLassoCV.alpha_\n",
    "cLasso = Lasso(alpha=cLassoCV.alpha_)\n",
    "cLasso.fit(x_train, y_train)\n",
    "print \"Training Error: %s\" %(1-cLasso.score(x_train, y_train))\n",
    "print \"Test Error: %s\" %(1-cLasso.score(x_test, y_test))\n",
    "\n",
    "print \"-\" * 70\n",
    "print \"Given that the test error for Ridge and Lasso are pretty close, I would choose the Lasso Regression because it allows for feature selection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.This time  we will try to predict the variable *Private* using the other variables in the College data set. The features and target variable are prepared for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = college.iloc[:, 1:]\n",
    "y = college.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) Split this data into a training set and a test set with train_size=0.5(Hint: Use the function **sklearn.cross_validation.train_test_split** , set *random_state=1* and *tran_size=0.5*.)]\n",
    "\n",
    "- (2) Fit a logistic regression with regularizaton. Use the function **GridSearchCV** to fint out the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid_para_logit = [{'penality': ['l1', 'l2'], 'alpha': np.logspace(-5, 5, 100), 'normalize':[True, False]}]\n",
    "grid_para_logit = [{'penalty': ['l1', 'l2'], 'C': np.logspace(-5, 5, 100)}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - What's the best parameters?\n",
    "    - Refit the model on the training set with best parameters. What's the training error and test error?\n",
    "    \n",
    "- (3) Fit a KNN model. Use the function **GridSearchCV** to fint out the appropriate parameter *n_neighbors*. Refit the model on the training set and report the training error and test error.\n",
    "\n",
    "- (4) Compare the results of logistic regression and KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Logistic Regression:\n",
      "Best Parameters:\n",
      "{'penalty': 'l1', 'C': 0.027185882427329403}\n",
      "Refitting model with best parameters...\n",
      "Training Error: 0.0438144329897\n",
      "Test Error: 0.0591259640103\n",
      "----------------------------------------------------------------------\n",
      "KNN:\n",
      "Best Parameters:\n",
      "{'n_neighbors': 24}\n",
      "Best Estimator:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=24, p=2,\n",
      "           weights='uniform')\n",
      "Refitting model with best parameters...\n",
      "Training Error: 0.0567010309278\n",
      "Test Error: 0.0668380462725\n",
      "----------------------------------------------------------------------\n",
      "Logistic regression performs better than KNN\n"
     ]
    }
   ],
   "source": [
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(x, y, test_size = 0.5, random_state = 1)\n",
    "print \"-\" * 70\n",
    "print \"Logistic Regression:\"\n",
    "cLogReg = LogisticRegression()\n",
    "grid = GridSearchCV(cLogReg, grid_para_logit, cv=5, scoring='accuracy')\n",
    "grid.fit(x_train2, y_train2)\n",
    "print \"Best Parameters:\"\n",
    "print grid.best_params_\n",
    "print \"Refitting model with best parameters...\"\n",
    "cLogRegBest = grid.best_estimator_.fit(x_train2, y_train2)\n",
    "print \"Training Error: %s\" %(1-cLogRegBest.score(x_train2, y_train2))\n",
    "print \"Test Error: %s\" %(1-cLogRegBest.score(x_test2, y_test2))\n",
    "\n",
    "print \"-\" * 70\n",
    "print \"KNN:\"\n",
    "knnParams = [{'n_neighbors':[x for x in range(3,50)]}]\n",
    "cKNN = KNeighborsClassifier()\n",
    "gridKNN = GridSearchCV(cKNN, knnParams, scoring='accuracy', cv=5).fit(x_train2, y_train2)\n",
    "print \"Best Parameters:\"\n",
    "print gridKNN.best_params_\n",
    "print \"Best Estimator:\"\n",
    "print gridKNN.best_estimator_\n",
    "print \"Refitting model with best parameters...\"\n",
    "cKNNBest = gridKNN.best_estimator_.fit(x_train2, y_train2)\n",
    "print \"Training Error: %s\" %(1-cKNNBest.score(x_train2, y_train2))\n",
    "print \"Test Error: %s\" %(1-cKNNBest.score(x_test2, y_test2))\n",
    "\n",
    "print \"-\" * 70\n",
    "print \"Logistic regression performs better than KNN\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
